---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am currently a third-year master’s student in the [Colledge of Computer Science](https://cc.nankai.edu.cn/) at [Nankai University](https://www.nankai.edu.cn/), supervised by [Prof. Jufeng Yang](https://cv.nankai.edu.cn/) in Computer Vision Lab. Before that, I received my bachelor’s degree in Computer Science and Technology from [Sichuan University](https://www.scu.edu.cn/) in 2022.

I am very fortunate to be co-advised by [Prof. Deng-Ping Fan](https://dengpingfan.github.io/) of Nankai University and [Peng Xu](https://www.pengxu.net/) of Tsinghua University.

My research interests primarily revolve around computer vision and machine learning, with a particular focus on camouflaged vision perception, mutimodal learning, and video understanding.

Feel free to reach out to me at the following email addresses:

  - zhaopancheng@mail.nankai.edu.cn (most commonly used)
  - zhao_pancheng@foxmail.com
  - pc.zhao99@gmail.com

<br>

Recent News
======

<ul style="padding-left: 40px; padding-bottom: 0px;">
  <li><font style="color:black; font-weight: bold;">[12/2024]</font> I receive the <font style="color:rgb(219,122,27); font-weight: bold;"> 2024 China National Scholarship (Master Student) </font>.</li>
  <li><font style="color:black; font-weight: bold;">[06/2024]</font> I'm going to <font style="color:rgb(219,122,27); font-weight: bold;">Alibaba (Hangzhou) </font> for a summer internship.</li>
  <li><font style="color:black; font-weight: bold;">[06/2024]</font> I’m going to Seattle for <font style="color:rgb(219,122,27); font-weight: bold;">CVPR 2024</font>.</li>
  <li><font style="color:black; font-weight: bold;">[05/2024]</font> I will be attending <font style="color:rgb(219,122,27); font-weight: bold;">VALSE 2024 (Chongqing) </font> and <font style="color:rgb(219,122,27); font-weight: bold;">CCIG 2024 (Xi‘an) </font> and doing poster sharing at the conference, so feel free to discuss with me. </li>
  <li><font style="color:black; font-weight: bold;">[02/2024]</font> Two papers for camouflaged image generation and video emotion analysis are accepted by <font style="color:rgb(219,122,27); font-weight: bold;">CVPR 2024</font>.</li>
  <li><font style="color:black; font-weight: bold;">[09/2022]</font> I start my Master studying at <font style="font-weight: bold;">Nankai University (NKU)</font> under the supervision of <font style="color:rgb(97,160,191); font-weight: bold;"><a href="https://cv.nankai.edu.cn/">Prof. Jufeng Yang</a></font>.</li>        
</ul>
<br>

Publications
======

<!-- <p>More publications can be found in <a href="">Google Scholar</a>.</p> -->

<!-- <p>† indicates equal contribution.</p> -->

<!-- Preprint CDP -->

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">Technical Report</div>
      <img src='papers/CDP/images/cdp_logo.png' width="100%">
    </div>
  </div>
  <div class='paper-box-text'>
		 <p><a href="/publication/CDP_Preprint">Deep Learning in Concealed Dense Prediction</a></p>


    <p><strong>Pancheng Zhao</strong>, Deng-Ping Fan, Shupeng Cheng, Salman Khan, Fahad Shahbaz Khan, David Clifton, Peng Xu, and Jufeng Yang</p>
    
    <p>Technical Report, April 16, 2025.</p>
    
    <a href="http://arxiv.org/abs/2504.10979"><img src='https://img.shields.io/badge/PDF-CDP-red' alt='Paper PDF'></a>	<a href=''><img src='https://img.shields.io/badge/Official Version-CDP-blue' alt='Project Page'></a>	
    <a href='https://github.com/PanchengZhao/Concealed-Dense-Prediction'><img src='https://img.shields.io/badge/Project Page-CDP-yellow' alt='Project Page'></a>
  </div>
</div>


<!-- LAKERED CVPR24 -->

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">CVPR 2024</div>
      <img src='papers/LAKERED/images/lakered_logo.jpg' width="100%">
    </div>
  </div>
  <div class='paper-box-text'>
		 <p><a href="/publication/LAKERED_CVPR24">LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion</a></p>


    <p><strong>Pancheng Zhao</strong>, Peng Xu, Pengda Qin, Deng-Ping Fan, Zhicheng Zhang, Guoli Jia, Bowen Zhou, Jufeng Yang</p>
    
    <p>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, USA, June 17-21, 2024.</p>
    
    <a href="https://arxiv.org/abs/2404.00292"><img src='https://img.shields.io/badge/PDF-LAKE RED-red' alt='Paper PDF'></a>	<a href=''><img src='https://img.shields.io/badge/Official Version-LAKE RED-blue' alt='Project Page'></a>	
    <a href='https://github.com/PanchengZhao/LAKE-RED'><img src='https://img.shields.io/badge/Project Page-LAKE RED-yellow' alt='Project Page'></a>
  </div>
</div>



<!-- MART CVPR24 -->

<div class='paper-box'>
  <div class='paper-box-image'>
    <div>
      <div class="badge">CVPR 2024</div>
      <img src='papers/MART/images/mart_logo.jpg' width="100%">
    </div>
  </div>
  <div class='paper-box-text'>
		 <p><a href="/publication/MART_CVPR24">MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation</a></p>

<!-- <p>Zhicheng Zhang<sup>†</sup>, <strong>Pancheng Zhao<sup>†</sup></strong>, Eunil Park, Jufeng Yang</p> -->
<p>Zhicheng Zhang, <strong>Pancheng Zhao</strong>, Eunil Park, Jufeng Yang</p>

<p>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, USA, June 17-21, 2024.</p>

<a href="papers/MART/Zhang_MART Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation_CVPR_2024_paper.pdf"><img src='https://img.shields.io/badge/PDF-MART-red' alt='Paper PDF'></a>	<a href=''><img src='https://img.shields.io/badge/Official Version-MART-blue' alt='Official Version'></a> <a href='https://zzcheng.top/MART'><img src='https://img.shields.io/badge/Project Page-MART-yellow' alt='Project Page'></a>	
<!-- <a href=''><img src='https://img.shields.io/badge/Chinese Version-MART-yellow' alt='Project Page'></a>	 -->

  </div>
</div>


<br>

Visitors
======

<div class='global_map'>
<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=1vZ7vYMjnqPq6rrJZomBNI5_1Bglli8x-Sk1eaOuEU8"></script>
</div>